{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4becd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PARMEET\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\PARMEET\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\PARMEET\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\PARMEET\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac74e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue_type       76\n",
      "urgency_level    52\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>urgency_level</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Payment issue for my SmartWatch V2. I was unde...</td>\n",
       "      <td>Billing Problem</td>\n",
       "      <td>Medium</td>\n",
       "      <td>SmartWatch V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I ordered SoundWave 300 but got EcoBreeze AC i...</td>\n",
       "      <td>Wrong Item</td>\n",
       "      <td>Medium</td>\n",
       "      <td>SoundWave 300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Facing installation issue with PhotoSnap Cam. ...</td>\n",
       "      <td>Installation Issue</td>\n",
       "      <td>Low</td>\n",
       "      <td>PhotoSnap Cam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Can you tell me more about the PhotoSnap Cam w...</td>\n",
       "      <td>General Inquiry</td>\n",
       "      <td>Medium</td>\n",
       "      <td>PhotoSnap Cam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>is malfunction. It stopped working after just...</td>\n",
       "      <td>Product Defect</td>\n",
       "      <td>Low</td>\n",
       "      <td>EcoBreeze AC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                        ticket_text  \\\n",
       "0          1  Payment issue for my SmartWatch V2. I was unde...   \n",
       "2          3  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
       "3          4  Facing installation issue with PhotoSnap Cam. ...   \n",
       "5          6  Can you tell me more about the PhotoSnap Cam w...   \n",
       "6          7   is malfunction. It stopped working after just...   \n",
       "\n",
       "           issue_type urgency_level        product  \n",
       "0     Billing Problem        Medium  SmartWatch V2  \n",
       "2          Wrong Item        Medium  SoundWave 300  \n",
       "3  Installation Issue           Low  PhotoSnap Cam  \n",
       "5     General Inquiry        Medium  PhotoSnap Cam  \n",
       "6      Product Defect           Low   EcoBreeze AC  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"ai_dev_assignment_tickets_complex_1000.xls\")\n",
    "print(df[['issue_type', 'urgency_level']].isnull().sum())\n",
    "df = df.dropna(subset=['issue_type', 'urgency_level'])\n",
    "\n",
    "df['ticket_text'] = df['ticket_text'].fillna('')\n",
    "# Check it's loaded\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0096dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# Hardcoded stopword list (simplified)\n",
    "stop_words = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"he\", \"him\", \"his\",\n",
    "    \"she\", \"her\", \"hers\", \"it\", \"its\", \"they\", \"them\", \"their\", \"theirs\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"a\", \"an\", \"the\",\n",
    "    \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "    \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\",\n",
    "    \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\n",
    "    \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\n",
    "    \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\"\n",
    "])\n",
    "\n",
    "# Simple Lemmatizer (fallback if WordNet fails)\n",
    "def basic_lemmatize(word):\n",
    "    # Very basic rules (for noun/plural, verb endings)\n",
    "    word = re.sub(r'(ing|ed|s)$', '', word)\n",
    "    return word\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [basic_lemmatize(w) for w in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca1a869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ticket_text  \\\n",
      "0  Payment issue for my SmartWatch V2. I was unde...   \n",
      "1  Can you tell me more about the UltraClean Vacu...   \n",
      "2  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
      "3  Facing installation issue with PhotoSnap Cam. ...   \n",
      "4  Order #30903 for Vision LED TV is 13 days late...   \n",
      "\n",
      "                                          clean_text  \n",
      "0         payment issue smartwatch v underbill order  \n",
      "1  tell ultraclean vacuum warranty also available...  \n",
      "2  order soundwave got ecobreeze ac instead order...  \n",
      "3  fac installation issue photosnap cam setup fai...  \n",
      "4  order vision l tv day late order march also co...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"ai_dev_assignment_tickets_complex_1000.xls\")\n",
    "\n",
    "df['clean_text'] = df['ticket_text'].apply(clean_text)\n",
    "\n",
    "# Check results\n",
    "print(df[['ticket_text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa63b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf328d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Score\n",
    "df['sentiment'] = df['ticket_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "# Ticket Length\n",
    "df['text_length'] = df['ticket_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# TF-IDF Features\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78102954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with custom features\n",
    "import numpy as np\n",
    "X = np.hstack((X_tfidf.toarray(),\n",
    "               df[['sentiment', 'text_length']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcd2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "issue_encoder = LabelEncoder()\n",
    "urgency_encoder = LabelEncoder()\n",
    "y_issue = issue_encoder.fit_transform(df['issue_type'])\n",
    "y_urgency = urgency_encoder.fit_transform(df['urgency_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62d6e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ML \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108b0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "# 0. Define a basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation/numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['ticket_text'].apply(clean_text)\n",
    "\n",
    "# 1. TF-IDF on cleaned text\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(df['clean_text'])\n",
    "\n",
    "# 2. Add extra features\n",
    "df['sentiment'] = df['ticket_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df['text_length'] = df['ticket_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# 3. Combine features\n",
    "X = np.hstack((X_tfidf.toarray(), df[['sentiment', 'text_length']].values))\n",
    "\n",
    "# 4. Encode targets\n",
    "issue_encoder = LabelEncoder()\n",
    "urgency_encoder = LabelEncoder()\n",
    "\n",
    "y_issue = issue_encoder.fit_transform(df['issue_type'])\n",
    "y_urgency = urgency_encoder.fit_transform(df['urgency_level'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72ec29ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Type Classification:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Account Access       0.91      0.94      0.92        32\n",
      "   Billing Problem       0.94      0.91      0.93        35\n",
      "   General Inquiry       0.71      0.96      0.82        26\n",
      "Installation Issue       0.96      0.89      0.92        27\n",
      "     Late Delivery       0.86      0.90      0.88        20\n",
      "    Product Defect       0.90      0.82      0.86        22\n",
      "        Wrong Item       0.87      0.95      0.91        21\n",
      "               nan       0.00      0.00      0.00        17\n",
      "\n",
      "          accuracy                           0.83       200\n",
      "         macro avg       0.77      0.80      0.78       200\n",
      "      weighted avg       0.81      0.83      0.82       200\n",
      "\n",
      "Urgency Level Classification:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.29      0.33      0.31        67\n",
      "         Low       0.32      0.27      0.29        64\n",
      "      Medium       0.25      0.25      0.25        61\n",
      "         nan       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.27       200\n",
      "   macro avg       0.22      0.21      0.21       200\n",
      "weighted avg       0.28      0.27      0.27       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_issue_train, y_issue_test = train_test_split(X, y_issue, test_size=0.2, random_state=42)\n",
    "_, _, y_urgency_train, y_urgency_test = train_test_split(X, y_urgency, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classifiers\n",
    "clf_issue = RandomForestClassifier()\n",
    "clf_issue.fit(X_train, y_issue_train)\n",
    "\n",
    "clf_urgency = RandomForestClassifier()\n",
    "clf_urgency.fit(X_train, y_urgency_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Issue Type Classification:\\n\", classification_report(y_issue_test, clf_issue.predict(X_test), target_names=issue_encoder.classes_.astype(str)))\n",
    "print(\"Urgency Level Classification:\\n\", classification_report(y_urgency_test, clf_urgency.predict(X_test), target_names=urgency_encoder.classes_.astype(str)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function extract_entities at 0x00000275A17F2CA0>\n"
     ]
    }
   ],
   "source": [
    "# Entity Extraction\n",
    "import dateutil.parser\n",
    "\n",
    "# Complaint Keywords (expand as needed)\n",
    "keywords = [\"broken\", \"error\", \"late\", \"missing\", \"damaged\", \"failed\", \"problem\"]\n",
    "\n",
    "# Product List (get from dataset column)\n",
    "product_list = df['product'].dropna().unique().tolist()\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = {\"product\": None, \"dates\": [], \"keywords\": []}\n",
    "    \n",
    "    # Product\n",
    "    for product in product_list:\n",
    "        if product and product.lower() in text.lower():\n",
    "            entities[\"product\"] = product\n",
    "            break\n",
    "\n",
    "    # Dates\n",
    "    found_dates = re.findall(r\"\\b(?:\\d{1,2}[-/th|st|nd|rd\\s]*)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[-/\\s]?\\d{2,4}\\b\", text, re.IGNORECASE)\n",
    "    entities[\"dates\"] = found_dates\n",
    "\n",
    "    # Keywords\n",
    "    entities[\"keywords\"] = [kw for kw in keywords if kw in text.lower()]\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a21ab4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration function\n",
    "def analyze_ticket(text):\n",
    "    clean = clean_text(text)\n",
    "    tfidf_vec = tfidf.transform([clean])\n",
    "    length = len(text.split())\n",
    "    sentiment = TextBlob(str(text)).sentiment.polarity\n",
    "    custom_feats = np.array([[sentiment, length]])\n",
    "    full_input = np.hstack((tfidf_vec.toarray(), custom_feats))\n",
    "    \n",
    "    issue_pred = issue_encoder.inverse_transform(clf_issue.predict(full_input))[0]\n",
    "    urgency_pred = urgency_encoder.inverse_transform(clf_urgency.predict(full_input))[0]\n",
    "    entities = extract_entities(text)\n",
    "    \n",
    "    return {\n",
    "        \"issue_type\": issue_pred,\n",
    "        \"urgency_level\": urgency_pred,\n",
    "        \"entities\": entities\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
